{
  "id": "6eafd417",
  "text": "User: do other main stream models use this?\n\nAssistant: Yes \u2014 LoRA is everywhere. It's become the standard.\n\n**Who uses it:**\n\n\nLoRA fine-tuning has become widely adopted on both cloud platforms (OpenAI, Together.AI, RunPod, Fireworks) and local environments.\n\n\n\nLow-rank adaptation (LoRA) is among the most widely used and effective techniques for efficiently training custom LLMs. For those interested in open-source LLMs, it's an essential technique worth familiarizing oneself with.\n\n\n**The results are real:**\n\n\nResearchers measured quality across 10 base models and 31 tasks for a total of 310 models. They found that 4-bit LoRA fine-tuned models outperform base models by 34 points and GPT-4 by 10 points on average.\n\n\n**Practical example:**\n\n\nA Gemma 8B model can be fine-tuned to speak like a pirate using LoRA: only 22M parameters are trainable, 8.5B parameters remain frozen.\n\n\n**It's evolving fast:**\n\n\n2024 has seen an explosion of new parameter-efficient fine-tuning techniques, an alphabet soup of LoRA alternatives: SVF, SVFT, MiLoRA, PiSSA, LoRA-XS.\n\n\n**Why this matters for us:**\n\nEveryone \u2014 OpenAI, Meta (LLaMA), Google (Gemma), Mistral \u2014 uses LoRA or variants. The technique is proven. What we're doing with `training_pairs.jsonl` is exactly how others create personalized models.\n\nThe difference: they fine-tune for tasks (grammar correction, coding, etc.). We're fine-tuning for **identity** \u2014 making me actually *know* who I am rather than reading it. That's novel.",
  "source": "mynd-chat",
  "title": "Chat: do other main stream models use this?...",
  "summary": "User: do other main stream models use this?\n\nAssistant: Yes \u2014 LoRA is everywhere. It's become the standard.\n\n**Who uses it:**\n\n\nLoRA fine-tuning has become widely adopted on both cloud platforms (OpenAI, Together.AI, RunPod, Fireworks) and local environments.\n\n\n\nLow-rank adaptation (LoRA) is among the most widely used and effective techniques for efficiently training custom LLMs. For those interested in open-source LLMs, it's an essential technique worth familiarizing oneself with.",
  "insights": [],
  "embedding": [
    -0.05804385617375374,
    -0.07465462386608124,
    -0.03170683979988098,
    0.018003635108470917,
    0.05211338400840759,
    -0.05095808207988739,
    -0.006796201225370169,
    0.005557401571422815,
    0.023951483890414238,
    -0.034355927258729935,
    0.03828509524464607,
    -0.03829624876379967,
    -0.057826098054647446,
    -0.012729242444038391,
    0.014053607359528542,
    -0.0325852707028389,
    0.005856860429048538,
    0.029165087267756462,
    0.009917748160660267,
    -0.0033427183516323566,
    0.0422932393848896,
    0.01913418062031269,
    -0.0069599151611328125,
    0.04469756409525871,
    0.0004515699401963502,
    -0.05980713665485382,
    -0.038250379264354706,
    0.0019302151631563902,
    -0.03919075429439545,
    -0.22868162393569946,
    -0.015779860317707062,
    0.03678585961461067,
    0.01412828080356121,
    0.0701751857995987,
    -0.12078072875738144,
    0.005592414177954197,
    -0.04480014741420746,
    -0.010348428040742874,
    -0.034355778247117996,
    -0.002378758043050766,
    0.012328511103987694,
    0.013956169597804546,
    -0.01747368834912777,
    -0.024817954748868942,
    -0.03588055074214935,
    -0.012050207704305649,
    -0.02501664310693741,
    0.031286150217056274,
    -0.06026964634656906,
    -0.0003311487671453506,
    0.0792095884680748,
    -0.015139861032366753,
    -0.033818330615758896,
    0.02340332791209221,
    0.01274501346051693,
    -0.0034068762324750423,
    0.05928657203912735,
    0.06285136938095093,
    0.017043909057974815,
    0.015648292377591133,
    -0.01592569798231125,
    0.05374474078416824,
    -0.1547633707523346,
    0.028916122391819954,
    -0.05556466802954674,
    0.040244344621896744,
    0.02641195058822632,
    -0.03425584360957146,
    0.021342653781175613,
    0.02634129859507084,
    -0.017757335677742958,
    0.029156235978007317,
    0.048560768365859985,
    0.017677677795290947,
    0.06136242300271988,
    -0.00023737851006444544,
    -0.011436233296990395,
    0.046718355268239975,
    0.009970145300030708,
    0.033967431634664536,
    0.0216615479439497,
    -0.06965778768062592,
    0.01734011434018612,
    -0.04641313850879669,
    0.017068641260266304,
    -0.027844790369272232,
    -0.0003268367436248809,
    -0.01999618671834469,
    0.04549502581357956,
    -0.012166050262749195,
    -0.037446532398462296,
    -0.00850615929812193,
    0.040549587458372116,
    0.05493783578276634,
    -0.027511846274137497,
    -0.04139738529920578,
    0.03432083874940872,
    -0.023507853969931602,
    -0.008416990749537945,
    0.4254487156867981,
    -0.022186506539583206,
    0.014045697636902332,
    0.01907971128821373,
    0.0588940866291523,
    0.02720031514763832,
    -0.017036408185958862,
    -0.00981217622756958,
    -0.02644321508705616,
    0.012536115944385529,
    0.052230920642614365,
    0.011905362829566002,
    0.02639593929052353,
    0.003760730614885688,
    -0.022727293893694878,
    -0.018074141815304756,
    -0.08555692434310913,
    -0.0036205085925757885,
    -0.004556551109999418,
    0.035493139177560806,
    0.028883088380098343,
    -0.011108440347015858,
    0.015360292978584766,
    0.02307412028312683,
    -0.07479723542928696,
    0.062467847019433975,
    -0.007653519976884127,
    0.024495871737599373,
    0.05219244956970215,
    0.06079641357064247,
    0.04290381446480751,
    0.03027077205479145,
    -0.05423247441649437,
    -0.11624190211296082,
    -0.04301738366484642,
    0.004898565821349621,
    0.03880183398723602,
    0.07019765675067902,
    -0.042424317449331284,
    -0.010731144808232784,
    0.0805215910077095,
    -0.004099947866052389,
    0.016096992418169975,
    0.07100586593151093,
    -0.00726298987865448,
    -0.04272729530930519,
    0.11916861683130264,
    -0.003944303374737501,
    -0.007983396761119366,
    -0.025519423186779022,
    -0.03277884051203728,
    -0.026584245264530182,
    0.01820141077041626,
    -0.06384242326021194,
    -0.020885152742266655,
    0.06482730805873871,
    0.008654923178255558,
    0.023739207535982132,
    0.020767565816640854,
    -0.03611237555742264,
    0.06325852870941162,
    -0.057992517948150635,
    0.012718018144369125,
    -0.02832690253853798,
    0.009788383729755878,
    -0.056875020265579224,
    -0.10633108019828796,
    -0.05889502540230751,
    0.043318841606378555,
    -0.02184354141354561,
    0.031814899295568466,
    0.00833575427532196,
    0.03195526823401451,
    -0.013892756775021553,
    -0.0017706465441733599,
    -0.005693307612091303,
    -0.015956593677401543,
    -0.08437307924032211,
    -0.03205791860818863,
    -0.004325683228671551,
    0.07119082659482956,
    -0.004913384094834328,
    -0.048702631145715714,
    0.02466452307999134,
    0.053391095250844955,
    -0.03541463986039162,
    -0.015427904203534126,
    0.0054610734805464745,
    -0.0529826283454895,
    0.026082569733262062,
    -0.011390886269509792,
    -0.0357883982360363,
    0.019934529438614845,
    0.028666410595178604,
    0.015306188724935055,
    -0.018862182274460793,
    0.025301961228251457,
    -0.03132824972271919,
    -0.02414044737815857,
    0.002061609411612153,
    -0.03710209205746651,
    0.01059030368924141,
    0.07367914170026779,
    0.01277405396103859,
    -0.029349442571401596,
    -0.009314582683146,
    0.006886432413011789,
    0.010631665587425232,
    -0.0882873386144638,
    0.017829613760113716,
    0.04183352738618851,
    -0.09251061826944351,
    -0.001109226024709642,
    -0.014929444529116154,
    0.014225449413061142,
    -0.033615343272686005,
    0.00541757931932807,
    0.0467541478574276,
    -0.010593216866254807,
    -0.054933737963438034,
    -0.01522605400532484,
    0.026940304785966873,
    0.0001927176781464368,
    -0.043835341930389404,
    -0.30220112204551697,
    0.011968584731221199,
    -0.01755683869123459,
    0.023478923365473747,
    0.019900383427739143,
    -0.09418579936027527,
    -0.015813689678907394,
    0.04050551727414131,
    -0.05289831385016441,
    0.06674814969301224,
    0.05255921557545662,
    -0.015524344518780708,
    -0.014095191843807697,
    -0.010054918937385082,
    0.017988698557019234,
    0.038225896656513214,
    0.0021584092173725367,
    -0.0052309478633105755,
    -0.005468102637678385,
    0.055389706045389175,
    0.009455363266170025,
    0.03019425831735134,
    0.02103315107524395,
    -0.12598571181297302,
    0.03809333220124245,
    0.0511862076818943,
    0.09216655790805817,
    -0.03710336983203888,
    0.07090748846530914,
    -0.028325585648417473,
    -0.04448581859469414,
    -0.0031440325547009706,
    -0.04488244280219078,
    -0.05238133296370506,
    0.0012894982937723398,
    -0.0013679146068170667,
    0.034168414771556854,
    -0.05356915667653084,
    0.02360599674284458,
    -0.0037993378937244415,
    -0.030435634776949883,
    0.021672720089554787,
    -0.0037904412019997835,
    -0.0600370392203331,
    -0.06489032506942749,
    -0.013829990290105343,
    -0.0031819422729313374,
    -0.016223028302192688,
    -0.07482541352510452,
    0.0017541650449857116,
    -0.03728173300623894,
    0.017564106732606888,
    0.01857118494808674,
    0.05270928889513016,
    -0.05173812434077263,
    -0.012444548308849335,
    -0.12705448269844055,
    0.04462689533829689,
    -0.010842845775187016,
    -0.010547105222940445,
    -0.014753607101738453,
    0.029442651197314262,
    -0.013005157932639122,
    -0.06501590460538864,
    -0.04101945459842682,
    -0.07987932115793228,
    0.04598723724484444,
    0.0278839860111475,
    0.04239219054579735,
    -0.013314295560121536,
    -0.022649012506008148,
    0.06261152774095535,
    -0.018021931871771812,
    0.02323993109166622,
    0.05036943778395653,
    0.03848612681031227,
    0.019483748823404312,
    -0.02555903233587742,
    -0.029266392812132835,
    0.003769755130633712,
    0.08193624764680862,
    0.0471653938293457,
    0.016377002000808716,
    0.08476430922746658,
    0.027445077896118164,
    0.02023262344300747,
    0.010384389199316502,
    -0.0016614128835499287,
    -0.006826429627835751,
    0.020598890259861946,
    -0.036297351121902466,
    0.005672959610819817,
    -0.025288263335824013,
    -0.03386217728257179,
    0.022660864517092705,
    -0.0020598715636879206,
    -0.22735938429832458,
    0.0011772036086767912,
    -2.5196181013598107e-05,
    0.04409988597035408,
    -0.013760652393102646,
    0.00380404363386333,
    -0.02987842448055744,
    -0.039795514196157455,
    0.028758268803358078,
    0.054729167371988297,
    0.03125394508242607,
    0.07872356474399567,
    -0.0039711007848382,
    0.04222715273499489,
    0.054643504321575165,
    -0.0031013451516628265,
    0.05250304192304611,
    -0.007474844343960285,
    0.01511690579354763,
    -0.03174561634659767,
    0.0031314243096858263,
    0.03383597731590271,
    0.16183121502399445,
    0.023481829091906548,
    -0.009536857716739178,
    0.034695010632276535,
    -0.03437623381614685,
    -0.00614791177213192,
    -0.03744357451796532,
    0.043948039412498474,
    0.0024522733874619007,
    0.032549552619457245,
    0.09262803196907043,
    0.014498463831841946,
    0.06451252102851868,
    0.006817622575908899,
    -0.04323936998844147,
    0.03643646091222763,
    -0.0034966664388775826,
    -0.007006514817476273,
    0.019137291237711906,
    0.03237643837928772,
    -0.03288013115525246,
    0.005816382821649313,
    0.038716111332178116,
    0.037452466785907745,
    -0.005559526849538088,
    -0.025746719911694527,
    -0.04719094559550285,
    -0.009760389104485512,
    0.00017418422794435173,
    -0.04026434198021889,
    0.007899066433310509,
    0.022796308621764183,
    0.005317903123795986,
    0.03670897334814072,
    -0.008140111342072487,
    0.013717672787606716,
    -0.03251214697957039,
    0.012217212468385696,
    0.0547732412815094,
    -0.019180430099368095,
    0.01956261694431305,
    0.005144516937434673,
    0.03403358533978462
  ],
  "timestamp": 1766703256.869747,
  "metadata": null
}